{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d9bbf86da5e"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99c1c3fc2ca5"
   },
   "source": [
    "# Vertex AI Model Garden - Llama 3 Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3de7470326a2"
   },
   "source": [
    "## Overview\n",
    "\n",
    "With growing public access to bigger and more capable Large Language Models (LLMs) such as Llama2 and Falcon, we're also seeing a growing trend of customers looking to adopt and customize (fine-tune) these foundational models for their specific use cases with custom datasets. By fine tuning these foundational models, customers can quickly turn these general purpose language models into domain experts;- allowing the models to respond with specific responses to user prompts and reduce the chance of hallucinated responses.\n",
    "\n",
    "\n",
    "This notebook demonstrates finetuning and deploying Llama 3 models with Vertex AI. The examples in this notebook use parameter efficient finetuning methods [PEFT (LoRA)](https://github.com/huggingface/peft) to reduce training and storage costs. LoRA (Low-Rank Adaptation) is one approach of Parameter Efficient FineTuning (PEFT), where pretrained model weights are frozen and rank decomposition matrices representing the change in model weights are trained during finetuning. Read more about LoRA in the following publication: [Hu, E.J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L. and Chen, W., 2021. Lora: Low-rank adaptation of large language models. *arXiv preprint arXiv:2106.09685*](https://arxiv.org/abs/2106.09685).\n",
    "\n",
    "After finetuning, we can deploy models on Vertex with GPU.\n",
    "\n",
    "\n",
    "### Objective\n",
    "\n",
    "- Finetune Llama 3 models with Vertex AI Custom Training Jobs.\n",
    "- Deploy finetuned Llama 3 models on Vertex AI Prediction.\n",
    "- Send prediction requests to your finetuned Llama 3 models.\n",
    "\n",
    "\n",
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage pricing](https://cloud.google.com/storage/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "264c07757582"
   },
   "source": [
    "## Install Vertex AI SDK for Python and other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install --upgrade --user google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Restart current runtime\n",
    "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Restart kernel after installs so that your environment can access the new packages\n",
    "import IPython\n",
    "import time\n",
    "\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define Google Cloud project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your project ID is: cloud-llm-preview3\n"
     ]
    }
   ],
   "source": [
    "# Define project information\n",
    "\n",
    "import sys\n",
    "\n",
    "GOOGLE_CLOUD_PROJECT = \"\"  # @param {type:\"string\"}\n",
    "GOOGLE_CLOUD_REGION = \"us-west1\"  # @param {type:\"string\"}\n",
    "\n",
    "# if not running on colab, try to get the PROJECT_ID automatically\n",
    "if \"google.colab\" not in sys.modules:\n",
    "    import subprocess\n",
    "\n",
    "    PROJECT_ID = subprocess.check_output(\n",
    "        [\"gcloud\", \"config\", \"get-value\", \"project\"], text=True\n",
    "    ).strip()\n",
    "\n",
    "print(f\"Your project ID is: {PROJECT_ID}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling API's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable the Vertex AI API and Compute Engine API, if not already.\n",
    "print(\"Enabling Vertex AI API and Compute Engine API.\")\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com\n",
    "\n",
    "# Enable Vertex AI and Cloud Compute APIs.\n",
    "! gcloud config set project $PROJECT_ID\n",
    "! gcloud services enable aiplatform.googleapis.com compute.googleapis.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initializing Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Initialize Vertex AI\n",
    "import vertexai\n",
    "\n",
    "vertexai.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "form",
    "id": "855d6b96f291",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling Vertex AI API and Compute Engine API.\n",
      "Operation \"operations/acat.p2-620568690313-f6f50017-178b-4d93-b424-a14d95a97855\" finished successfully.\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n",
      "Creating gs://cloud-llm-preview3-tmp-20240506031756/...\n",
      "Using this GCS Bucket: gs://cloud-llm-preview3-tmp-20240506031756\n",
      "Using this default Service Account: 620568690313-compute@developer.gserviceaccount.com\n",
      "Initializing Vertex AI API.\n",
      "Updated property [core/project].\n",
      "Operation \"operations/acat.p2-620568690313-8d02ce3b-d17c-4000-9e3d-0347391f33b6\" finished successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "from typing import Tuple\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Get the default cloud project id.\n",
    "PROJECT_ID = GOOGLE_CLOUD_PROJECT\n",
    "\n",
    "# Get the default region for launching jobs.\n",
    "REGION = GOOGLE_CLOUD_REGION\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating GCS Bucket for Model Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Storage bucket for storing the experiment artifacts.\n",
    "# A unique GCS bucket will be created for the purpose of this notebook. If you\n",
    "# prefer using your own GCS bucket, change the value yourself below.\n",
    "now = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "BUCKET_URI = \"gs://\"  # @param {type:\"string\"}\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "assert BUCKET_URI.startswith(\"gs://\"), \"BUCKET_URI must start with `gs://`.\"\n",
    "if BUCKET_URI is None or BUCKET_URI.strip() == \"\" or BUCKET_URI == \"gs://\":\n",
    "    # Create a unique GCS bucket for this notebook, if not specified by the user\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-tmp-{now}\"\n",
    "    ! gsutil mb -l {REGION} {BUCKET_URI}\n",
    "else:\n",
    "    shell_output = ! gsutil ls -Lb {BUCKET_NAME} | grep \"Location constraint:\" | sed \"s/Location constraint://\"\n",
    "    bucket_region = shell_output[0].strip().lower()\n",
    "    if bucket_region != REGION:\n",
    "        raise ValueError(\n",
    "            \"Bucket region %s is different from notebook region %s\"\n",
    "            % (bucket_region, REGION)\n",
    "        )\n",
    "\n",
    "print(f\"Using this GCS Bucket: {BUCKET_URI}\")\n",
    "\n",
    "# Gets the default BUCKET_URI and SERVICE_ACCOUNT if they were not specified by the user.\n",
    "shell_output = ! gcloud projects describe $PROJECT_ID\n",
    "project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "print(\"Using this default Service Account:\", SERVICE_ACCOUNT)\n",
    "\n",
    "# Provision permissions to the SERVICE_ACCOUNT with the GCS bucket\n",
    "BUCKET_NAME = \"/\".join(BUCKET_URI.split(\"/\")[:3])\n",
    "! gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_NAME\n",
    "\n",
    "# Initialize Vertex AI API.\n",
    "STAGING_BUCKET = os.path.join(BUCKET_URI, \"staging\")\n",
    "MODEL_BUCKET = os.path.join(STAGING_BUCKET, \"model\")\n",
    "print(\"Initializing Vertex AI API.\")\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=STAGING_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Llama 3 models\n",
    "\n",
    "We have an option for GPU based finetuning and serving, choose between accessing Llama 3 models on [Hugging Face](https://huggingface.co/) or Vertex AI as described below.\n",
    "\n",
    "If you already obtained access to Llama 3 models on [Hugging Face](https://huggingface.co/), you can load models from there.\n",
    "Alternatively, you can also load the original Llama 3 models for finetuning and serving from Vertex AI after accepting the agreement.\n",
    "\n",
    "## Note : Its important to accept the agreements on both Hugging Face and Vertex AI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "form",
    "id": "36c21f10355f",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MODEL_BUCKET' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 34\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LOAD_MODEL_FROM \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGoogle Cloud\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m     28\u001b[0m         VERTEX_AI_MODEL_GARDEN_LLAMA3\n\u001b[1;32m     29\u001b[0m     ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClick the agreement of Llama 3 in Vertex AI Model Garden, and get the GCS path of Llama 3 model artifacts.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[1;32m     31\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCopying Llama 3 model artifacts from\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m         VERTEX_AI_MODEL_GARDEN_LLAMA3,\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 34\u001b[0m         \u001b[43mMODEL_BUCKET\u001b[49m,\n\u001b[1;32m     35\u001b[0m     )\n\u001b[1;32m     36\u001b[0m     HF_TOKEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# ! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA3/* $MODEL_BUCKET\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MODEL_BUCKET' is not defined"
     ]
    }
   ],
   "source": [
    "# Select one of the following sections.**\n",
    "# LOAD_MODEL_FROM = \"Hugging Face\"  # @param [\"Hugging Face\", \"Google Cloud\"] {isTemplate:true}\n",
    "LOAD_MODEL_FROM = \"Google Cloud\"\n",
    "\n",
    "\n",
    "# You must provide a Hugging Face User Access Token (read) to access the Llama 3 models. You can follow the [Hugging Face documentation](https://huggingface.co/docs/hub/en/security-tokens) to create a **read** access token and put it in the `HF_TOKEN` field below.\n",
    "\n",
    "# HF_TOKEN = \"\"  # @param {type:\"string\", isTemplate:true}\n",
    "# if LOAD_MODEL_FROM == \"Hugging Face\":\n",
    "#     assert (\n",
    "#         HF_TOKEN\n",
    "#     ), \"Provide a read HF_TOKEN to load models from Hugging Face, or select a different model source.\"\n",
    "\n",
    "\n",
    "## Access Llama 3 models on Vertex AI for GPU based serving\n",
    "## The original models from Meta are converted into the Hugging Face format for serving in Vertex AI.\n",
    "## Accept the model agreement to access the models:\n",
    "\n",
    "# 1. Open the [Llama 3 model card](https://console.cloud.google.com/vertex-ai/publishers/meta/model-garden/llama3) from [Vertex AI Model Garden](https://cloud.google.com/model-garden).\n",
    "# 2. Review and accept the agreement in the pop-up window on the model card page. If you have previously accepted the model agreement, there will not be a pop-up window on the model card page and this step is not needed.\n",
    "# 3. After accepting the agreement of Llama 3, a `gs://` URI containing Llama 3 pretrained and finetuned models will be shared.\n",
    "# 4. Paste the URI in the `VERTEX_AI_MODEL_GARDEN_LLAMA3` field below.\n",
    "\n",
    "VERTEX_AI_MODEL_GARDEN_LLAMA3 = \"gs://vertex-model-garden-public-us/llama3\"  # @param {type:\"string\", isTemplate:true}\n",
    "\n",
    "if LOAD_MODEL_FROM == \"Google Cloud\":\n",
    "    assert (\n",
    "        VERTEX_AI_MODEL_GARDEN_LLAMA3\n",
    "    ), \"Click the agreement of Llama 3 in Vertex AI Model Garden, and get the GCS path of Llama 3 model artifacts.\"\n",
    "    print(\n",
    "        \"Copying Llama 3 model artifacts from\",\n",
    "        VERTEX_AI_MODEL_GARDEN_LLAMA3,\n",
    "        \"to \",\n",
    "        MODEL_BUCKET,\n",
    "    )\n",
    "    HF_TOKEN = \"\"\n",
    "\n",
    "    # ! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA3/* $MODEL_BUCKET\n",
    "    ! gsutil -m cp -R $VERTEX_AI_MODEL_GARDEN_LLAMA3/llama3-8b-hf/* $MODEL_BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pre-built training and Serving Docker Images\n",
    "The training image uses transformers 4.38.2 and tokenizers 0.15.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-peft-train:20240415_0936_RC00\"\n",
    "VLLM_DOCKER_URI = \"us-docker.pkg.dev/vertex-ai/vertex-vision-model-garden-dockers/pytorch-vllm-serve:20240418_0936_RC01\"\n",
    "\n",
    "\n",
    "def get_job_name_with_datetime(prefix: str) -> str:\n",
    "    \"\"\"Gets the job name with date time when triggering training or deployment\n",
    "    jobs in Vertex AI.\n",
    "    \"\"\"\n",
    "    return prefix + datetime.now().strftime(\"_%Y%m%d_%H%M%S\")\n",
    "\n",
    "\n",
    "def deploy_model(\n",
    "    model_name: str,\n",
    "    model_id: str,\n",
    "    service_account: str,\n",
    "    machine_type: str = \"g2-standard-8\",\n",
    "    accelerator_type: str = \"NVIDIA_L4\",\n",
    "    accelerator_count: int = 1,\n",
    "    gpu_memory_utilization: float = 0.9,\n",
    "    max_model_len: int = 4096,\n",
    ") -> Tuple[aiplatform.Model, aiplatform.Endpoint]:\n",
    "    \"\"\"Deploys trained models with vLLM into Vertex AI.\"\"\"\n",
    "    endpoint = aiplatform.Endpoint.create(display_name=f\"{model_name}-endpoint\")\n",
    "\n",
    "    vllm_args = [\n",
    "        \"--host=0.0.0.0\",\n",
    "        \"--port=7080\",\n",
    "        f\"--model={model_id}\",\n",
    "        f\"--tensor-parallel-size={accelerator_count}\",\n",
    "        \"--swap-space=16\",\n",
    "        f\"--gpu-memory-utilization={gpu_memory_utilization}\",\n",
    "        f\"--max-model-len={max_model_len}\",\n",
    "        \"--disable-log-stats\",\n",
    "    ]\n",
    "\n",
    "    env_vars = {\"MODEL_ID\": model_id}\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=model_name,\n",
    "        serving_container_image_uri=VLLM_DOCKER_URI,\n",
    "        serving_container_command=[\"python\", \"-m\", \"vllm.entrypoints.api_server\"],\n",
    "        serving_container_args=vllm_args,\n",
    "        serving_container_ports=[7080],\n",
    "        serving_container_predict_route=\"/generate\",\n",
    "        serving_container_health_route=\"/ping\",\n",
    "        serving_container_environment_variables=env_vars,\n",
    "    )\n",
    "    print(\n",
    "        f\"Deploying {model_name} on {machine_type} with {accelerator_count} {accelerator_type} GPU(s).\"\n",
    "    )\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        deploy_request_timeout=1800,\n",
    "        service_account=service_account,\n",
    "    )\n",
    "    print(\"endpoint_name:\", endpoint.name)\n",
    "\n",
    "    return model, endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cb56d402e84a"
   },
   "source": [
    "## Finetune with HuggingFace PEFT and deploy with vLLM on GPUs\n",
    "Use the Vertex AI SDK to create and run the custom training jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cellView": "form",
    "id": "KwAW99YZHTdy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# This notebook uses [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset as an example.\n",
    "# You can set `dataset_name` to any existing [Hugging Face dataset](https://huggingface.co/datasets) name, and set `instruct_column_in_dataset` to the name of the dataset column containing training data. The [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) has only one column `text`, and therefore we set `instruct_column_in_dataset` to `text` in this notebook.\n",
    "\n",
    "# (Optional) Prepare a custom JSONL dataset for finetuning\n",
    "\n",
    "# You can prepare a JSONL file where each line is a valid JSON string as your custom training dataset. For example, here is one line from the [timdettmers/openassistant-guanaco](https://huggingface.co/datasets/timdettmers/openassistant-guanaco) dataset:\n",
    "# ```\n",
    "#  {\"text\": \"### Human: Hola### Assistant: \\u00a1Hola! \\u00bfEn qu\\u00e9 puedo ayudarte hoy?\"}\n",
    "# ```\n",
    "\n",
    "# Hugging Face dataset name or gs:// URI to a custom JSONL dataset.\n",
    "dataset_name = \"timdettmers/openassistant-guanaco\"  # @param {type:\"string\"}\n",
    "\n",
    "# Name of the dataset column containing training text input.\n",
    "instruct_column_in_dataset = \"text\"  # @param {type:\"string\"}\n",
    "\n",
    "# Optional. Template name or gs:// URI to a custom template.\n",
    "template = \"\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Finetune : Setup Training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ivVGS9dHXPOz",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://cloud-llm-preview3-tmp-20240506031756/staging/aiplatform-custom-training-2024-05-06-03:37:02.722 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west1/training/5507052421785845760?project=620568690313\n",
      "CustomContainerTrainingJob projects/620568690313/locations/us-west1/trainingPipelines/5507052421785845760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west1/training/4377352602757627904?project=620568690313\n",
      "CustomContainerTrainingJob projects/620568690313/locations/us-west1/trainingPipelines/5507052421785845760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/620568690313/locations/us-west1/trainingPipelines/5507052421785845760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/620568690313/locations/us-west1/trainingPipelines/5507052421785845760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/620568690313/locations/us-west1/trainingPipelines/5507052421785845760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/620568690313/locations/us-west1/trainingPipelines/5507052421785845760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/620568690313/locations/us-west1/trainingPipelines/5507052421785845760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/620568690313/locations/us-west1/trainingPipelines/5507052421785845760 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "# Use the Vertex AI SDK to create and run the custom training jobs.\n",
    "# We recommend setting `finetuning_precision_mode` to `4bit` because it enables using fewer hardware resources for finetuning.\n",
    "# The Llama 3 base model.\n",
    "\n",
    "MODEL_ID = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # @param [\"meta-llama/Meta-Llama-3-8B\", \"meta-llama/Meta-Llama-3-8B-Instruct\", \"meta-llama/Meta-Llama-3-70B\", \"meta-llama/Meta-Llama-3-70B-Instruct\"] {isTemplate:true}\n",
    "if LOAD_MODEL_FROM == \"Google Cloud\":\n",
    "    if MODEL_ID == \"meta-llama/Meta-Llama-3-8B\":\n",
    "        base_model_id = \"llama3-8b-hf\"\n",
    "    elif MODEL_ID == \"meta-llama/Meta-Llama-3-8B-Instruct\":\n",
    "        base_model_id = \"llama3-8b-chat-hf\"\n",
    "    elif MODEL_ID == \"meta-llama/Meta-Llama-3-70B\":\n",
    "        base_model_id = \"llama3-70b-hf\"\n",
    "    elif MODEL_ID == \"meta-llama/Meta-Llama-3-70B-Instruct\":\n",
    "        base_model_id = \"llama3-70b-chat-hf\"\n",
    "    else:\n",
    "        raise ValueError(f\"Undefined model ID: {MODEL_ID}.\")\n",
    "    base_model_id = os.path.join(MODEL_BUCKET, base_model_id)\n",
    "else:\n",
    "    base_model_id = MODEL_ID\n",
    "\n",
    "# The accelerator to use.\n",
    "accelerator_type = \"NVIDIA_L4\"  # @param [\"NVIDIA_L4\", \"NVIDIA_TESLA_A100\"]\n",
    "\n",
    "# Batch size for finetuning.\n",
    "per_device_train_batch_size = 1  # @param{type:\"integer\"}\n",
    "# Runs 10 training steps as a minimal example.\n",
    "max_steps = 10  # @param {type:\"integer\"}\n",
    "# Precision mode for finetuning.\n",
    "finetuning_precision_mode = \"4bit\"  # @param [\"4bit\", \"8bit\", \"float16\"]\n",
    "# Learning rate.\n",
    "learning_rate = 2e-4  # @param{type:\"number\"}\n",
    "# LoRA parameters.\n",
    "lora_rank = 16  # @param{type:\"integer\"}\n",
    "lora_alpha = 64  # @param{type:\"integer\"}\n",
    "lora_dropout = 0.1  # @param{type:\"number\"}\n",
    "# Maximum sequence length.\n",
    "max_seq_length = 8192\n",
    "\n",
    "# Worker pool spec.\n",
    "\n",
    "machine_type = None\n",
    "if \"8b\" in MODEL_ID.lower():\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        if finetuning_precision_mode == \"4bit\" or finetuning_precision_mode == \"8bit\":\n",
    "            accelerator_count = 1\n",
    "            machine_type = \"g2-standard-12\"\n",
    "        else:\n",
    "            accelerator_count = 2\n",
    "            machine_type = \"g2-standard-24\"\n",
    "    elif accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        accelerator_count = 1\n",
    "        machine_type = \"a2-highgpu-1g\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Recommended machine settings not found for: {accelerator_type}. To use another accelerator, edit this code block to pass in an appropriate `machine_type`, `accelerator_type`, and `accelerator_count` to the deploy_model function by clicking `Show Code` and then modifying the code.\"\n",
    "        )\n",
    "elif \"70b\" in MODEL_ID.lower():\n",
    "    if accelerator_type == \"NVIDIA_L4\":\n",
    "        accelerator_count = 8\n",
    "        machine_type = \"g2-standard-96\"\n",
    "    elif accelerator_type == \"NVIDIA_TESLA_A100\":\n",
    "        accelerator_count = 4\n",
    "        machine_type = \"a2-highgpu-4g\"\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Recommended machine settings not found for: {accelerator_type}. To use another accelerator, edit this code block to pass in an appropriate `machine_type`, `accelerator_type`, and `accelerator_count` to the deploy_model function by clicking `Show Code` and then modifying the code.\"\n",
    "        )\n",
    "else:\n",
    "    raise ValueError(f\"Unsupported model ID or GCS path: {MODEL_ID}.\")\n",
    "\n",
    "replica_count = 1\n",
    "\n",
    "# Setup training job.\n",
    "job_name = get_job_name_with_datetime(\"llama3-lora-train\")\n",
    "\n",
    "# Pass training arguments and launch job.\n",
    "train_job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=job_name,\n",
    "    container_uri=TRAIN_DOCKER_URI,\n",
    ")\n",
    "\n",
    "# Create a GCS folder to store the LORA adapter.\n",
    "lora_adapter_dir = get_job_name_with_datetime(\"llama3-lora-adapter\")\n",
    "lora_output_dir = os.path.join(STAGING_BUCKET, lora_adapter_dir)\n",
    "\n",
    "# Create a GCS folder to store the merged model with the base model and the\n",
    "# finetuned LORA adapter.\n",
    "merged_model_dir = get_job_name_with_datetime(\"llama3-merged-model\")\n",
    "merged_model_output_dir = os.path.join(STAGING_BUCKET, merged_model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trigger training Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_job.run(\n",
    "    args=[\n",
    "        \"--task=instruct-lora\",\n",
    "        f\"--pretrained_model_id={base_model_id}\",\n",
    "        f\"--dataset_name={dataset_name}\",\n",
    "        f\"--instruct_column_in_dataset={instruct_column_in_dataset}\",\n",
    "        f\"--output_dir={lora_output_dir}\",\n",
    "        f\"--merge_base_and_lora_output_dir={merged_model_output_dir}\",\n",
    "        f\"--per_device_train_batch_size={per_device_train_batch_size}\",\n",
    "        f\"--lora_rank={lora_rank}\",\n",
    "        f\"--lora_alpha={lora_alpha}\",\n",
    "        f\"--lora_dropout={lora_dropout}\",\n",
    "        f\"--max_steps={max_steps}\",\n",
    "        f\"--max_seq_length={max_seq_length}\",\n",
    "        f\"--learning_rate={learning_rate}\",\n",
    "        f\"--precision_mode={finetuning_precision_mode}\",\n",
    "        f\"--template={template}\",\n",
    "        f\"--huggingface_access_token={HF_TOKEN}\",\n",
    "    ],\n",
    "    environment_variables={\"WANDB_DISABLED\": True},\n",
    "    replica_count=replica_count,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    boot_disk_size_gb=500,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    ")\n",
    "\n",
    "print(\"LoRA adapter was saved in: \", lora_output_dir)\n",
    "print(\"Trained and merged models were saved in: \", merged_model_output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy of the Model to Vertex AI Endpoint\n",
    "This section uploads the model to Model Registry and deploys it on the Endpoint. It takes 15 minutes to 1 hour to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "qmHW6m8xG_4U",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deploying models in:  gs://cloud-llm-preview3-tmp-20240506031756/staging/llama3-merged-model_20240506_033702\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/620568690313/locations/us-west1/endpoints/4094409778023890944/operations/8839834893295812608\n",
      "Endpoint created. Resource name: projects/620568690313/locations/us-west1/endpoints/4094409778023890944\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/620568690313/locations/us-west1/endpoints/4094409778023890944')\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/620568690313/locations/us-west1/models/4325381986647539712/operations/512679182287765504\n",
      "Model created. Resource name: projects/620568690313/locations/us-west1/models/4325381986647539712@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/620568690313/locations/us-west1/models/4325381986647539712@1')\n",
      "Deploying llama3-vllm-serve_20240506_155759 on g2-standard-12 with 1 NVIDIA_L4 GPU(s).\n",
      "Deploying model to Endpoint : projects/620568690313/locations/us-west1/endpoints/4094409778023890944\n",
      "Deploy Endpoint model backing LRO: projects/620568690313/locations/us-west1/endpoints/4094409778023890944/operations/8692904955452850176\n",
      "Endpoint model deployed. Resource name: projects/620568690313/locations/us-west1/endpoints/4094409778023890944\n",
      "endpoint_name: 4094409778023890944\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"Deploying models in: \", merged_model_output_dir)\n",
    "\n",
    "# Find Vertex AI prediction supported accelerators and regions in [here](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute).\n",
    "if \"8b\" in MODEL_ID.lower():\n",
    "    machine_type = \"g2-standard-12\"\n",
    "    accelerator_type = \"NVIDIA_L4\"\n",
    "    accelerator_count = 1\n",
    "else:\n",
    "    machine_type = \"g2-standard-96\"\n",
    "    accelerator_type = \"NVIDIA_L4\"\n",
    "    accelerator_count = 8\n",
    "\n",
    "gpu_memory_utilization = 0.85\n",
    "max_model_len = 8192  # Maximum context length.\n",
    "\n",
    "model, endpoint = deploy_model(\n",
    "    model_name=get_job_name_with_datetime(prefix=\"llama3-vllm-serve\"),\n",
    "    model_id=merged_model_output_dir,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    machine_type=machine_type,\n",
    "    accelerator_type=accelerator_type,\n",
    "    accelerator_count=accelerator_count,\n",
    "    gpu_memory_utilization=gpu_memory_utilization,\n",
    "    max_model_len=max_model_len,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.models.Model object at 0x7fdc5112d120> \n",
      "resource name: projects/620568690313/locations/us-west1/models/4325381986647539712\n",
      "<google.cloud.aiplatform.models.Endpoint object at 0x7fdc519d0b20> \n",
      "resource name: projects/620568690313/locations/us-west1/endpoints/4094409778023890944\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "Once deployment succeeds, you can send requests to the endpoint with text prompts. Sampling parameters supported by vLLM can be found [here](https://docs.vllm.ai/en/latest/dev/sampling_params.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cellView": "form",
    "id": "2UYUNn60G_4U",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt:\n",
      "What is a car?\n",
      "Output:\n",
      " A car is a vehicle that is powered by an internal combustion engine and is designed to transport people or goods from one place to another. It is typically made of metal and has four wheels, a body, and a chassis. Cars are used for a\n"
     ]
    }
   ],
   "source": [
    "#  ```\n",
    "# Human: What is a car?\n",
    "# Assistant:  A car, or a motor car, is a road-connected human-transportation system used to move people or goods from one place to another. The term also encompasses a wide range of vehicles, including motorboats, trains, and aircrafts. Cars typically have four wheels, a cabin for passengers, and an engine or motor. They have been around since the early 19th century and are now one of the most popular forms of transportation, used for daily commuting, shopping, and other purposes.\n",
    "# ```\n",
    "\n",
    "\n",
    "prompt = \"What is a car?\"  # @param {type: \"string\"}\n",
    "max_tokens = 50  # @param {type:\"integer\"}\n",
    "temperature = 1.0  # @param {type:\"number\"}\n",
    "top_p = 1.0  # @param {type:\"number\"}\n",
    "top_k = 1  # @param {type:\"integer\"}\n",
    "raw_response = False  # @param {type:\"boolean\"}\n",
    "\n",
    "# Overides parameters for inferences.\n",
    "# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,\n",
    "# you can reduce the maximum number of output tokens, such as set max_tokens as 20.\n",
    "\n",
    "instances = [\n",
    "    {\n",
    "        \"prompt\": prompt,\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"raw_response\": raw_response,\n",
    "    },\n",
    "]\n",
    "response = endpoint.predict(instances=instances)\n",
    "\n",
    "for prediction in response.predictions:\n",
    "    print(prediction)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af21a3cff1e0"
   },
   "source": [
    "## Clean up resources\n",
    "Delete the model and endpoint to recycle the resources and avoid unnecessary continuous charges that may incur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "911406c1561e"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_job.delete()\n",
    "\n",
    "# Undeploy model and delete endpoint.\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete model.\n",
    "model.delete()\n",
    "\n",
    "delete_bucket = False  # @param {type:\"boolean\"}\n",
    "if delete_bucket:\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "model_garden_pytorch_llama3_finetuning.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m119",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m119"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
